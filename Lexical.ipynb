{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Grammar does not cover some of the input words: \"('I', 'PRP'), ('saw', 'VBD'), ('her', 'PRP'), ('duck', 'NN')\".",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mp:\\College stuff\\TY-1\\SEM2\\SP\\SP CP\\Lexical.ipynb Cell 2\u001b[0m in \u001b[0;36m<cell line: 18>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/p%3A/College%20stuff/TY-1/SEM2/SP/SP%20CP/Lexical.ipynb#W1sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m grammar \u001b[39m=\u001b[39m nltk\u001b[39m.\u001b[39mCFG\u001b[39m.\u001b[39mfromstring(\u001b[39m\"\"\"\u001b[39m\n\u001b[0;32m     <a href='vscode-notebook-cell:/p%3A/College%20stuff/TY-1/SEM2/SP/SP%20CP/Lexical.ipynb#W1sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m \u001b[39m    S -> NP VP\u001b[39m\n\u001b[0;32m     <a href='vscode-notebook-cell:/p%3A/College%20stuff/TY-1/SEM2/SP/SP%20CP/Lexical.ipynb#W1sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m \u001b[39m    NP -> N | DT N | NP PP\u001b[39m\n\u001b[0;32m     <a href='vscode-notebook-cell:/p%3A/College%20stuff/TY-1/SEM2/SP/SP%20CP/Lexical.ipynb#W1sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m \u001b[39m    VP -> V | V NP | V NP PP\u001b[39m\n\u001b[0;32m     <a href='vscode-notebook-cell:/p%3A/College%20stuff/TY-1/SEM2/SP/SP%20CP/Lexical.ipynb#W1sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m \u001b[39m    PP -> P NP\u001b[39m\n\u001b[0;32m     <a href='vscode-notebook-cell:/p%3A/College%20stuff/TY-1/SEM2/SP/SP%20CP/Lexical.ipynb#W1sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m \u001b[39m\"\"\"\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/p%3A/College%20stuff/TY-1/SEM2/SP/SP%20CP/Lexical.ipynb#W1sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m parser \u001b[39m=\u001b[39m nltk\u001b[39m.\u001b[39mChartParser(grammar)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/p%3A/College%20stuff/TY-1/SEM2/SP/SP%20CP/Lexical.ipynb#W1sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m \u001b[39mfor\u001b[39;00m tree \u001b[39min\u001b[39;00m parser\u001b[39m.\u001b[39;49mparse(tagged):\n\u001b[0;32m     <a href='vscode-notebook-cell:/p%3A/College%20stuff/TY-1/SEM2/SP/SP%20CP/Lexical.ipynb#W1sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m     \u001b[39mprint\u001b[39m(tree)\n\u001b[0;32m     <a href='vscode-notebook-cell:/p%3A/College%20stuff/TY-1/SEM2/SP/SP%20CP/Lexical.ipynb#W1sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m tokenized_text \u001b[39m=\u001b[39m sent_tokenize(text)\n",
      "File \u001b[1;32mp:\\python\\lib\\site-packages\\nltk\\parse\\chart.py:1474\u001b[0m, in \u001b[0;36mChartParser.parse\u001b[1;34m(self, tokens, tree_class)\u001b[0m\n\u001b[0;32m   1473\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mparse\u001b[39m(\u001b[39mself\u001b[39m, tokens, tree_class\u001b[39m=\u001b[39mTree):\n\u001b[1;32m-> 1474\u001b[0m     chart \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mchart_parse(tokens)\n\u001b[0;32m   1475\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39miter\u001b[39m(chart\u001b[39m.\u001b[39mparses(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_grammar\u001b[39m.\u001b[39mstart(), tree_class\u001b[39m=\u001b[39mtree_class))\n",
      "File \u001b[1;32mp:\\python\\lib\\site-packages\\nltk\\parse\\chart.py:1432\u001b[0m, in \u001b[0;36mChartParser.chart_parse\u001b[1;34m(self, tokens, trace)\u001b[0m\n\u001b[0;32m   1429\u001b[0m trace_new_edges \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_trace_new_edges\n\u001b[0;32m   1431\u001b[0m tokens \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(tokens)\n\u001b[1;32m-> 1432\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_grammar\u001b[39m.\u001b[39;49mcheck_coverage(tokens)\n\u001b[0;32m   1433\u001b[0m chart \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_chart_class(tokens)\n\u001b[0;32m   1434\u001b[0m grammar \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_grammar\n",
      "File \u001b[1;32mp:\\python\\lib\\site-packages\\nltk\\grammar.py:665\u001b[0m, in \u001b[0;36mCFG.check_coverage\u001b[1;34m(self, tokens)\u001b[0m\n\u001b[0;32m    663\u001b[0m \u001b[39mif\u001b[39;00m missing:\n\u001b[0;32m    664\u001b[0m     missing \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m, \u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mjoin(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mw\u001b[39m!r}\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39mfor\u001b[39;00m w \u001b[39min\u001b[39;00m missing)\n\u001b[1;32m--> 665\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    666\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mGrammar does not cover some of the \u001b[39m\u001b[39m\"\u001b[39m \u001b[39m\"\u001b[39m\u001b[39minput words: \u001b[39m\u001b[39m%r\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m missing\n\u001b[0;32m    667\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: Grammar does not cover some of the input words: \"('I', 'PRP'), ('saw', 'VBD'), ('her', 'PRP'), ('duck', 'NN')\"."
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "text = input(\"Enter String: \")\n",
    "import nltk\n",
    "\n",
    "\n",
    "# Use NLTK to generate the parse tree\n",
    "tokens = nltk.word_tokenize(text)\n",
    "tagged = nltk.pos_tag(tokens)\n",
    "grammar = nltk.CFG.fromstring(\"\"\"\n",
    "    S -> NP VP\n",
    "    NP -> N | DT N | NP PP\n",
    "    VP -> V | V NP | V NP PP\n",
    "    PP -> P NP\n",
    "\"\"\")\n",
    "parser = nltk.ChartParser(grammar)\n",
    "for tree in parser.parse(tagged):\n",
    "    print(tree)\n",
    "\n",
    "\n",
    "\n",
    "tokenized_text = sent_tokenize(text)\n",
    "tokenized_word = word_tokenize(text)\n",
    "# print(tokenized_text)\n",
    "# print(tokenized_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('I', 'PRP'), ('saw', 'VBD'), ('her', 'PRP'), ('duck', 'NN')]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_words = tokenized_word\n",
    "nltk.pos_tag(pos_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I is a homograph\n",
      "saw is a homograph\n",
      "duck is a homograph\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import wordnet\n",
    "\n",
    "# Define a function to check if a word is a homograph\n",
    "def is_homograph(word):\n",
    "    synsets = wordnet.synsets(word)\n",
    "    if len(synsets) > 1:\n",
    "        # Check if there are synsets with different meanings\n",
    "        meanings = set([s.definition() for s in synsets])\n",
    "        if len(meanings) > 1:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "for word in tokenized_word:\n",
    "    if is_homograph(word):\n",
    "        print(word + \" is a homograph\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'duck', 'her', 'saw', 'I'}\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "expected string or bytes-like object",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mp:\\College stuff\\TY-1\\SEM2\\SP\\SP CP\\Lexical.ipynb Cell 5\u001b[0m in \u001b[0;36m<cell line: 9>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/p%3A/College%20stuff/TY-1/SEM2/SP/SP%20CP/Lexical.ipynb#W4sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39mprint\u001b[39m(meanings)\n\u001b[0;32m      <a href='vscode-notebook-cell:/p%3A/College%20stuff/TY-1/SEM2/SP/SP%20CP/Lexical.ipynb#W4sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m \u001b[39m# Use the NLTK library to generate the parse tree\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/p%3A/College%20stuff/TY-1/SEM2/SP/SP%20CP/Lexical.ipynb#W4sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m tokens \u001b[39m=\u001b[39m nltk\u001b[39m.\u001b[39;49mword_tokenize(meanings)\n\u001b[0;32m     <a href='vscode-notebook-cell:/p%3A/College%20stuff/TY-1/SEM2/SP/SP%20CP/Lexical.ipynb#W4sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m tagged \u001b[39m=\u001b[39m nltk\u001b[39m.\u001b[39mpos_tag(tokens)\n\u001b[0;32m     <a href='vscode-notebook-cell:/p%3A/College%20stuff/TY-1/SEM2/SP/SP%20CP/Lexical.ipynb#W4sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m grammar \u001b[39m=\u001b[39m nltk\u001b[39m.\u001b[39mCFG\u001b[39m.\u001b[39mfromstring(\u001b[39m\"\"\"\u001b[39m\n\u001b[0;32m     <a href='vscode-notebook-cell:/p%3A/College%20stuff/TY-1/SEM2/SP/SP%20CP/Lexical.ipynb#W4sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m \u001b[39m    S -> NP VP\u001b[39m\n\u001b[0;32m     <a href='vscode-notebook-cell:/p%3A/College%20stuff/TY-1/SEM2/SP/SP%20CP/Lexical.ipynb#W4sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m \u001b[39m    NP -> DT NN\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/p%3A/College%20stuff/TY-1/SEM2/SP/SP%20CP/Lexical.ipynb#W4sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m \u001b[39m    V -> \u001b[39m\u001b[39m'\u001b[39m\u001b[39mchased\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m     <a href='vscode-notebook-cell:/p%3A/College%20stuff/TY-1/SEM2/SP/SP%20CP/Lexical.ipynb#W4sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m \u001b[39m\"\"\"\u001b[39m)\n",
      "File \u001b[1;32mp:\\python\\lib\\site-packages\\nltk\\tokenize\\__init__.py:129\u001b[0m, in \u001b[0;36mword_tokenize\u001b[1;34m(text, language, preserve_line)\u001b[0m\n\u001b[0;32m    114\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mword_tokenize\u001b[39m(text, language\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39menglish\u001b[39m\u001b[39m\"\u001b[39m, preserve_line\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m):\n\u001b[0;32m    115\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    116\u001b[0m \u001b[39m    Return a tokenized copy of *text*,\u001b[39;00m\n\u001b[0;32m    117\u001b[0m \u001b[39m    using NLTK's recommended word tokenizer\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    127\u001b[0m \u001b[39m    :type preserve_line: bool\u001b[39;00m\n\u001b[0;32m    128\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 129\u001b[0m     sentences \u001b[39m=\u001b[39m [text] \u001b[39mif\u001b[39;00m preserve_line \u001b[39melse\u001b[39;00m sent_tokenize(text, language)\n\u001b[0;32m    130\u001b[0m     \u001b[39mreturn\u001b[39;00m [\n\u001b[0;32m    131\u001b[0m         token \u001b[39mfor\u001b[39;00m sent \u001b[39min\u001b[39;00m sentences \u001b[39mfor\u001b[39;00m token \u001b[39min\u001b[39;00m _treebank_word_tokenizer\u001b[39m.\u001b[39mtokenize(sent)\n\u001b[0;32m    132\u001b[0m     ]\n",
      "File \u001b[1;32mp:\\python\\lib\\site-packages\\nltk\\tokenize\\__init__.py:107\u001b[0m, in \u001b[0;36msent_tokenize\u001b[1;34m(text, language)\u001b[0m\n\u001b[0;32m     97\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m     98\u001b[0m \u001b[39mReturn a sentence-tokenized copy of *text*,\u001b[39;00m\n\u001b[0;32m     99\u001b[0m \u001b[39musing NLTK's recommended sentence tokenizer\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    104\u001b[0m \u001b[39m:param language: the model name in the Punkt corpus\u001b[39;00m\n\u001b[0;32m    105\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    106\u001b[0m tokenizer \u001b[39m=\u001b[39m load(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mtokenizers/punkt/\u001b[39m\u001b[39m{\u001b[39;00mlanguage\u001b[39m}\u001b[39;00m\u001b[39m.pickle\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m--> 107\u001b[0m \u001b[39mreturn\u001b[39;00m tokenizer\u001b[39m.\u001b[39;49mtokenize(text)\n",
      "File \u001b[1;32mp:\\python\\lib\\site-packages\\nltk\\tokenize\\punkt.py:1276\u001b[0m, in \u001b[0;36mPunktSentenceTokenizer.tokenize\u001b[1;34m(self, text, realign_boundaries)\u001b[0m\n\u001b[0;32m   1272\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mtokenize\u001b[39m(\u001b[39mself\u001b[39m, text, realign_boundaries\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m):\n\u001b[0;32m   1273\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   1274\u001b[0m \u001b[39m    Given a text, returns a list of the sentences in that text.\u001b[39;00m\n\u001b[0;32m   1275\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1276\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mlist\u001b[39m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msentences_from_text(text, realign_boundaries))\n",
      "File \u001b[1;32mp:\\python\\lib\\site-packages\\nltk\\tokenize\\punkt.py:1332\u001b[0m, in \u001b[0;36mPunktSentenceTokenizer.sentences_from_text\u001b[1;34m(self, text, realign_boundaries)\u001b[0m\n\u001b[0;32m   1325\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39msentences_from_text\u001b[39m(\u001b[39mself\u001b[39m, text, realign_boundaries\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m):\n\u001b[0;32m   1326\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   1327\u001b[0m \u001b[39m    Given a text, generates the sentences in that text by only\u001b[39;00m\n\u001b[0;32m   1328\u001b[0m \u001b[39m    testing candidate sentence breaks. If realign_boundaries is\u001b[39;00m\n\u001b[0;32m   1329\u001b[0m \u001b[39m    True, includes in the sentence closing punctuation that\u001b[39;00m\n\u001b[0;32m   1330\u001b[0m \u001b[39m    follows the period.\u001b[39;00m\n\u001b[0;32m   1331\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1332\u001b[0m     \u001b[39mreturn\u001b[39;00m [text[s:e] \u001b[39mfor\u001b[39;00m s, e \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mspan_tokenize(text, realign_boundaries)]\n",
      "File \u001b[1;32mp:\\python\\lib\\site-packages\\nltk\\tokenize\\punkt.py:1332\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m   1325\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39msentences_from_text\u001b[39m(\u001b[39mself\u001b[39m, text, realign_boundaries\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m):\n\u001b[0;32m   1326\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   1327\u001b[0m \u001b[39m    Given a text, generates the sentences in that text by only\u001b[39;00m\n\u001b[0;32m   1328\u001b[0m \u001b[39m    testing candidate sentence breaks. If realign_boundaries is\u001b[39;00m\n\u001b[0;32m   1329\u001b[0m \u001b[39m    True, includes in the sentence closing punctuation that\u001b[39;00m\n\u001b[0;32m   1330\u001b[0m \u001b[39m    follows the period.\u001b[39;00m\n\u001b[0;32m   1331\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1332\u001b[0m     \u001b[39mreturn\u001b[39;00m [text[s:e] \u001b[39mfor\u001b[39;00m s, e \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mspan_tokenize(text, realign_boundaries)]\n",
      "File \u001b[1;32mp:\\python\\lib\\site-packages\\nltk\\tokenize\\punkt.py:1322\u001b[0m, in \u001b[0;36mPunktSentenceTokenizer.span_tokenize\u001b[1;34m(self, text, realign_boundaries)\u001b[0m\n\u001b[0;32m   1320\u001b[0m \u001b[39mif\u001b[39;00m realign_boundaries:\n\u001b[0;32m   1321\u001b[0m     slices \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_realign_boundaries(text, slices)\n\u001b[1;32m-> 1322\u001b[0m \u001b[39mfor\u001b[39;00m sentence \u001b[39min\u001b[39;00m slices:\n\u001b[0;32m   1323\u001b[0m     \u001b[39myield\u001b[39;00m (sentence\u001b[39m.\u001b[39mstart, sentence\u001b[39m.\u001b[39mstop)\n",
      "File \u001b[1;32mp:\\python\\lib\\site-packages\\nltk\\tokenize\\punkt.py:1421\u001b[0m, in \u001b[0;36mPunktSentenceTokenizer._realign_boundaries\u001b[1;34m(self, text, slices)\u001b[0m\n\u001b[0;32m   1408\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   1409\u001b[0m \u001b[39mAttempts to realign punctuation that falls after the period but\u001b[39;00m\n\u001b[0;32m   1410\u001b[0m \u001b[39mshould otherwise be included in the same sentence.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1418\u001b[0m \u001b[39m    [\"(Sent1.)\", \"Sent2.\"].\u001b[39;00m\n\u001b[0;32m   1419\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   1420\u001b[0m realign \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[1;32m-> 1421\u001b[0m \u001b[39mfor\u001b[39;00m sentence1, sentence2 \u001b[39min\u001b[39;00m _pair_iter(slices):\n\u001b[0;32m   1422\u001b[0m     sentence1 \u001b[39m=\u001b[39m \u001b[39mslice\u001b[39m(sentence1\u001b[39m.\u001b[39mstart \u001b[39m+\u001b[39m realign, sentence1\u001b[39m.\u001b[39mstop)\n\u001b[0;32m   1423\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m sentence2:\n",
      "File \u001b[1;32mp:\\python\\lib\\site-packages\\nltk\\tokenize\\punkt.py:318\u001b[0m, in \u001b[0;36m_pair_iter\u001b[1;34m(iterator)\u001b[0m\n\u001b[0;32m    316\u001b[0m iterator \u001b[39m=\u001b[39m \u001b[39miter\u001b[39m(iterator)\n\u001b[0;32m    317\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 318\u001b[0m     prev \u001b[39m=\u001b[39m \u001b[39mnext\u001b[39;49m(iterator)\n\u001b[0;32m    319\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mStopIteration\u001b[39;00m:\n\u001b[0;32m    320\u001b[0m     \u001b[39mreturn\u001b[39;00m\n",
      "File \u001b[1;32mp:\\python\\lib\\site-packages\\nltk\\tokenize\\punkt.py:1395\u001b[0m, in \u001b[0;36mPunktSentenceTokenizer._slices_from_text\u001b[1;34m(self, text)\u001b[0m\n\u001b[0;32m   1393\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_slices_from_text\u001b[39m(\u001b[39mself\u001b[39m, text):\n\u001b[0;32m   1394\u001b[0m     last_break \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[1;32m-> 1395\u001b[0m     \u001b[39mfor\u001b[39;00m match, context \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_match_potential_end_contexts(text):\n\u001b[0;32m   1396\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtext_contains_sentbreak(context):\n\u001b[0;32m   1397\u001b[0m             \u001b[39myield\u001b[39;00m \u001b[39mslice\u001b[39m(last_break, match\u001b[39m.\u001b[39mend())\n",
      "File \u001b[1;32mp:\\python\\lib\\site-packages\\nltk\\tokenize\\punkt.py:1375\u001b[0m, in \u001b[0;36mPunktSentenceTokenizer._match_potential_end_contexts\u001b[1;34m(self, text)\u001b[0m\n\u001b[0;32m   1373\u001b[0m before_words \u001b[39m=\u001b[39m {}\n\u001b[0;32m   1374\u001b[0m matches \u001b[39m=\u001b[39m []\n\u001b[1;32m-> 1375\u001b[0m \u001b[39mfor\u001b[39;00m match \u001b[39min\u001b[39;00m \u001b[39mreversed\u001b[39m(\u001b[39mlist\u001b[39m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_lang_vars\u001b[39m.\u001b[39;49mperiod_context_re()\u001b[39m.\u001b[39;49mfinditer(text))):\n\u001b[0;32m   1376\u001b[0m     \u001b[39m# Ignore matches that have already been captured by matches to the right of this match\u001b[39;00m\n\u001b[0;32m   1377\u001b[0m     \u001b[39mif\u001b[39;00m matches \u001b[39mand\u001b[39;00m match\u001b[39m.\u001b[39mend() \u001b[39m>\u001b[39m before_start:\n\u001b[0;32m   1378\u001b[0m         \u001b[39mcontinue\u001b[39;00m\n",
      "\u001b[1;31mTypeError\u001b[0m: expected string or bytes-like object"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "wn = nltk.WordNetLemmatizer()\n",
    "meanings = set(wn.lemmatize(w, 'n') for w in tokenized_word)\n",
    "print(meanings)\n",
    "meanings = [m for m in meanings if nltk.corpus.wordnet.synsets(m, pos='n')]\n",
    "print(meanings)\n",
    "\n",
    "def get_word_meanings(words):\n",
    "    synsets = wordnet.synsets(words)\n",
    "    meanings = set()\n",
    "\n",
    "    for synset in synsets:\n",
    "        for lemma in synset.lemmas():\n",
    "            meanings.add(lemma.name())\n",
    "\n",
    "    if len(meanings) > 1:\n",
    "        return list(meanings)\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "for words in meanings:\n",
    "    dm = get_word_meanings(words)\n",
    "    print(dm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'duck', 'her', 'saw', 'I'}\n",
      "['duck', 'saw', 'I']\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "not enough values to unpack (expected 3, got 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mp:\\College stuff\\TY-1\\SEM2\\SP\\SP CP\\Lexical.ipynb Cell 6\u001b[0m in \u001b[0;36m<cell line: 7>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/p%3A/College%20stuff/TY-1/SEM2/SP/SP%20CP/Lexical.ipynb#W5sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39m# If there is more than one meaning, choose the most common one\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/p%3A/College%20stuff/TY-1/SEM2/SP/SP%20CP/Lexical.ipynb#W5sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(meanings) \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m      <a href='vscode-notebook-cell:/p%3A/College%20stuff/TY-1/SEM2/SP/SP%20CP/Lexical.ipynb#W5sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m     \u001b[39m# Define a list of possible meanings in order of frequency\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/p%3A/College%20stuff/TY-1/SEM2/SP/SP%20CP/Lexical.ipynb#W5sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m     meanings_frequency \u001b[39m=\u001b[39m [(meaning, \u001b[39mlen\u001b[39m(nltk\u001b[39m.\u001b[39mcorpus\u001b[39m.\u001b[39mwordnet\u001b[39m.\u001b[39msynset(meaning)\u001b[39m.\u001b[39mlemmas())) \u001b[39mfor\u001b[39;00m meaning \u001b[39min\u001b[39;00m meanings]\n\u001b[0;32m     <a href='vscode-notebook-cell:/p%3A/College%20stuff/TY-1/SEM2/SP/SP%20CP/Lexical.ipynb#W5sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m     meanings_frequency\u001b[39m.\u001b[39msort(key\u001b[39m=\u001b[39m\u001b[39mlambda\u001b[39;00m x: x[\u001b[39m1\u001b[39m], reverse\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/p%3A/College%20stuff/TY-1/SEM2/SP/SP%20CP/Lexical.ipynb#W5sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m     \u001b[39m# Replace the ambiguous word with the most frequent meaning\u001b[39;00m\n",
      "\u001b[1;32mp:\\College stuff\\TY-1\\SEM2\\SP\\SP CP\\Lexical.ipynb Cell 6\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/p%3A/College%20stuff/TY-1/SEM2/SP/SP%20CP/Lexical.ipynb#W5sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39m# If there is more than one meaning, choose the most common one\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/p%3A/College%20stuff/TY-1/SEM2/SP/SP%20CP/Lexical.ipynb#W5sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(meanings) \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m      <a href='vscode-notebook-cell:/p%3A/College%20stuff/TY-1/SEM2/SP/SP%20CP/Lexical.ipynb#W5sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m     \u001b[39m# Define a list of possible meanings in order of frequency\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/p%3A/College%20stuff/TY-1/SEM2/SP/SP%20CP/Lexical.ipynb#W5sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m     meanings_frequency \u001b[39m=\u001b[39m [(meaning, \u001b[39mlen\u001b[39m(nltk\u001b[39m.\u001b[39;49mcorpus\u001b[39m.\u001b[39;49mwordnet\u001b[39m.\u001b[39;49msynset(meaning)\u001b[39m.\u001b[39mlemmas())) \u001b[39mfor\u001b[39;00m meaning \u001b[39min\u001b[39;00m meanings]\n\u001b[0;32m     <a href='vscode-notebook-cell:/p%3A/College%20stuff/TY-1/SEM2/SP/SP%20CP/Lexical.ipynb#W5sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m     meanings_frequency\u001b[39m.\u001b[39msort(key\u001b[39m=\u001b[39m\u001b[39mlambda\u001b[39;00m x: x[\u001b[39m1\u001b[39m], reverse\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/p%3A/College%20stuff/TY-1/SEM2/SP/SP%20CP/Lexical.ipynb#W5sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m     \u001b[39m# Replace the ambiguous word with the most frequent meaning\u001b[39;00m\n",
      "File \u001b[1;32mp:\\python\\lib\\site-packages\\nltk\\corpus\\reader\\wordnet.py:1434\u001b[0m, in \u001b[0;36mWordNetCorpusReader.synset\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   1432\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39msynset\u001b[39m(\u001b[39mself\u001b[39m, name):\n\u001b[0;32m   1433\u001b[0m     \u001b[39m# split name into lemma, part of speech and synset number\u001b[39;00m\n\u001b[1;32m-> 1434\u001b[0m     lemma, pos, synset_index_str \u001b[39m=\u001b[39m name\u001b[39m.\u001b[39mlower()\u001b[39m.\u001b[39mrsplit(\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m2\u001b[39m)\n\u001b[0;32m   1435\u001b[0m     synset_index \u001b[39m=\u001b[39m \u001b[39mint\u001b[39m(synset_index_str) \u001b[39m-\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m   1437\u001b[0m     \u001b[39m# get the offset for this synset\u001b[39;00m\n",
      "\u001b[1;31mValueError\u001b[0m: not enough values to unpack (expected 3, got 1)"
     ]
    }
   ],
   "source": [
    "wn = nltk.WordNetLemmatizer()\n",
    "meanings = set(wn.lemmatize(w, 'n') for w in tokenized_word)\n",
    "print(meanings)\n",
    "meanings = [m for m in meanings if nltk.corpus.wordnet.synsets(m, pos='n')]\n",
    "print(meanings)\n",
    "# If there is more than one meaning, choose the most common one\n",
    "if len(meanings) > 1:\n",
    "    # Define a list of possible meanings in order of frequency\n",
    "    meanings_frequency = [(meaning, len(nltk.corpus.wordnet.synset(meaning).lemmas())) for meaning in meanings]\n",
    "    meanings_frequency.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    # Replace the ambiguous word with the most frequent meaning\n",
    "    disambiguated_sentence = text.replace(tokenized_word[tokenized_word.index(list(meanings)[0])], meanings_frequency[0][0])\n",
    "else:\n",
    "    disambiguated_sentence = text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Grammar does not cover some of the input words: \"('I', 'PRP'), ('saw', 'VBD'), ('her', 'PRP'), ('cat', 'NN')\".",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mp:\\College stuff\\TY-1\\SEM2\\SP\\SP CP\\Lexical.ipynb Cell 7\u001b[0m in \u001b[0;36m<cell line: 20>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/p%3A/College%20stuff/TY-1/SEM2/SP/SP%20CP/Lexical.ipynb#X10sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m grammar \u001b[39m=\u001b[39m nltk\u001b[39m.\u001b[39mCFG\u001b[39m.\u001b[39mfromstring(\u001b[39m\"\"\"\u001b[39m\n\u001b[0;32m     <a href='vscode-notebook-cell:/p%3A/College%20stuff/TY-1/SEM2/SP/SP%20CP/Lexical.ipynb#X10sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m \u001b[39m    S -> NP VP\u001b[39m\n\u001b[0;32m     <a href='vscode-notebook-cell:/p%3A/College%20stuff/TY-1/SEM2/SP/SP%20CP/Lexical.ipynb#X10sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m \u001b[39m    NP -> N | DT N | NP PP\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/p%3A/College%20stuff/TY-1/SEM2/SP/SP%20CP/Lexical.ipynb#X10sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m \u001b[39m    DT -> \u001b[39m\u001b[39m'\u001b[39m\u001b[39mthe\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m     <a href='vscode-notebook-cell:/p%3A/College%20stuff/TY-1/SEM2/SP/SP%20CP/Lexical.ipynb#X10sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m \u001b[39m\"\"\"\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/p%3A/College%20stuff/TY-1/SEM2/SP/SP%20CP/Lexical.ipynb#X10sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m parser \u001b[39m=\u001b[39m nltk\u001b[39m.\u001b[39mChartParser(grammar)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/p%3A/College%20stuff/TY-1/SEM2/SP/SP%20CP/Lexical.ipynb#X10sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m \u001b[39mfor\u001b[39;00m tree \u001b[39min\u001b[39;00m parser\u001b[39m.\u001b[39;49mparse(tagged):\n\u001b[0;32m     <a href='vscode-notebook-cell:/p%3A/College%20stuff/TY-1/SEM2/SP/SP%20CP/Lexical.ipynb#X10sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m     \u001b[39mprint\u001b[39m(tree)\n",
      "File \u001b[1;32mp:\\python\\lib\\site-packages\\nltk\\parse\\chart.py:1474\u001b[0m, in \u001b[0;36mChartParser.parse\u001b[1;34m(self, tokens, tree_class)\u001b[0m\n\u001b[0;32m   1473\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mparse\u001b[39m(\u001b[39mself\u001b[39m, tokens, tree_class\u001b[39m=\u001b[39mTree):\n\u001b[1;32m-> 1474\u001b[0m     chart \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mchart_parse(tokens)\n\u001b[0;32m   1475\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39miter\u001b[39m(chart\u001b[39m.\u001b[39mparses(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_grammar\u001b[39m.\u001b[39mstart(), tree_class\u001b[39m=\u001b[39mtree_class))\n",
      "File \u001b[1;32mp:\\python\\lib\\site-packages\\nltk\\parse\\chart.py:1432\u001b[0m, in \u001b[0;36mChartParser.chart_parse\u001b[1;34m(self, tokens, trace)\u001b[0m\n\u001b[0;32m   1429\u001b[0m trace_new_edges \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_trace_new_edges\n\u001b[0;32m   1431\u001b[0m tokens \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(tokens)\n\u001b[1;32m-> 1432\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_grammar\u001b[39m.\u001b[39;49mcheck_coverage(tokens)\n\u001b[0;32m   1433\u001b[0m chart \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_chart_class(tokens)\n\u001b[0;32m   1434\u001b[0m grammar \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_grammar\n",
      "File \u001b[1;32mp:\\python\\lib\\site-packages\\nltk\\grammar.py:665\u001b[0m, in \u001b[0;36mCFG.check_coverage\u001b[1;34m(self, tokens)\u001b[0m\n\u001b[0;32m    663\u001b[0m \u001b[39mif\u001b[39;00m missing:\n\u001b[0;32m    664\u001b[0m     missing \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m, \u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mjoin(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mw\u001b[39m!r}\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39mfor\u001b[39;00m w \u001b[39min\u001b[39;00m missing)\n\u001b[1;32m--> 665\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    666\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mGrammar does not cover some of the \u001b[39m\u001b[39m\"\u001b[39m \u001b[39m\"\u001b[39m\u001b[39minput words: \u001b[39m\u001b[39m%r\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m missing\n\u001b[0;32m    667\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: Grammar does not cover some of the input words: \"('I', 'PRP'), ('saw', 'VBD'), ('her', 'PRP'), ('cat', 'NN')\"."
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "# Prompt the user to input a sentence\n",
    "sentence = input(\"Enter a sentence: \")\n",
    "\n",
    "# Use NLTK to generate the parse tree\n",
    "tokens = nltk.word_tokenize(sentence)\n",
    "tagged = nltk.pos_tag(tokens)\n",
    "grammar = nltk.CFG.fromstring(\"\"\"\n",
    "    S -> NP VP\n",
    "    NP -> N | DT N | NP PP\n",
    "    VP -> V | V NP | V NP PP\n",
    "    PP -> P NP\n",
    "    N -> 'John' | 'cat' | 'roof'\n",
    "    V -> 'saw'\n",
    "    P -> 'on'\n",
    "    DT -> 'the'\n",
    "\"\"\")\n",
    "parser = nltk.ChartParser(grammar)\n",
    "for tree in parser.parse(tagged):\n",
    "    print(tree)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "919242494a6cc0e904c821135dcd072446e0b99c010270adb9c3d3efacc8bf58"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
