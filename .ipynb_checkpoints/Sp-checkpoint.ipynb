{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter a sentence: i saw her duck\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.tree import ParentedTree\n",
    "from nltk import word_tokenize, pos_tag, RegexpParser\n",
    "from nltk.tree import Tree\n",
    "\n",
    "sentence = input(\"Enter a sentence: \")\n",
    "\n",
    "# Tokenize the sentence and extract the part of speech tags\n",
    "tokens = word_tokenize(sentence)\n",
    "pos_tags = pos_tag(tokens)\n",
    "\n",
    "words = nltk.word_tokenize(sentence)\n",
    "\n",
    "# Identify the part-of-speech tags for the words\n",
    "pos_tags = nltk.pos_tag(words)\n",
    "\n",
    "# Define the grammar for the parser\n",
    "grammar = r\"\"\"\n",
    "    NP: {<DT|PP\\$>?<JJ>*<NN>}   # chunk determiner/possessive, adjectives and noun\n",
    "        {<NNP>+}                # chunk sequences of proper nouns\n",
    "        {<PRP>}\n",
    "    VP: {<VB.*>+}               # chunk verbs and their modals\n",
    "        {<JJ>*<RB>?<VB.*>+}     # chunk adverbs modifying the verb\n",
    "    PP: {<IN><NP>}              # chunk prepositions followed by noun phrases\n",
    "\"\"\"\n",
    "\n",
    "# Create the parser and parse the sentence\n",
    "parser = RegexpParser(grammar)\n",
    "tree = parser.parse(pos_tags)\n",
    "\n",
    "# Visualize the parse tree\n",
    "tree.draw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('i', 'NN')\n",
      "('saw', 'VBD')\n",
      "('her', 'PRP')\n",
      "('duck', 'NN')\n"
     ]
    }
   ],
   "source": [
    "for i in pos_tags:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'spacy'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mspacy\u001b[39;00m\n\u001b[0;32m      3\u001b[0m nlp \u001b[38;5;241m=\u001b[39m spacy\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124men_core_web_sm\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      4\u001b[0m text \u001b[38;5;241m=\u001b[39m sentence\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'spacy'"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "text = sentence\n",
    "doc = nlp(text)\n",
    "\n",
    "# Getting dependency tags\n",
    "for token in doc:\n",
    "    print(token.text, '=>', token.dep_)\n",
    "\n",
    "# Importing visualizer\n",
    "from spacy import displacy\n",
    "\n",
    "# Visualizing dependency tree\n",
    "displacy.render(doc, jupyter=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aw = []\n",
    "ambiguous_words = []\n",
    "sentences = []\n",
    "\n",
    "for i, (word, pos) in enumerate(pos_tags):\n",
    "    w=\"\"\n",
    "    if pos == 'NN' or pos == 'VB':\n",
    "        # Check if the word is a homograph (similar words with a different meaning)\n",
    "        \n",
    "        if len(nltk.corpus.wordnet.synsets(word)) > 1:\n",
    "            print(\"The word '{}' is ambiguous in the sentence '{}'.\".format(\n",
    "                word, sentence))\n",
    "            aw.append(word)\n",
    "            w=word\n",
    "\n",
    "        # Identify the possible meanings for any verbs or nouns with multiple senses\n",
    "        meanings = set()\n",
    "        for synset in wordnet.synsets(word):\n",
    "            for lemma in synset.lemmas():\n",
    "                meanings.add(lemma.name())\n",
    "        if len(meanings) > 1:\n",
    "            print(f\"Ambiguous word: {word}\")\n",
    "            for j, meaning in enumerate(meanings):\n",
    "                ambiguous_words.append(meaning)\n",
    "                print(f\"  Meaning {j+1}: {meaning}\") \n",
    "    # Generate all possible sentences by replacing ambiguous words\n",
    "    for meanings in ambiguous_words:\n",
    "            new_sentence = sentence.replace(word, meanings)\n",
    "            sentences.append(new_sentence) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "She had a bust\n",
      "She had a shoot_down\n",
      "She had a displume\n",
      "She had a bout\n",
      "She had a rupture\n",
      "She had a split\n",
      "She had a teardrop\n",
      "She had a rent\n",
      "She had a rip\n",
      "She had a pluck\n",
      "She had a snag\n",
      "She had a tear\n",
      "She had a deplume\n",
      "She had a deplumate\n",
      "She had a pull\n",
      "She had a binge\n",
      "She had a buck\n",
      "She had a snap\n",
      "She had a charge\n",
      "She had a shoot\n"
     ]
    }
   ],
   "source": [
    "for i in sentences:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import pos_tag\n",
    "\n",
    "# grammar = ('''\n",
    "#     NP: {<NNP><VBZ|VBD><DT><JJ>*<NN><.>}\n",
    "#     ''')\n",
    "\n",
    "# grammar = r\"\"\"\n",
    "#     NP: {<DT>?<JJ>*<NN.*>+}       # noun phrase\n",
    "#     VP: {<VB.*><NP|PP>*}          # verb phrase\n",
    "#     PP: {<IN><NP>}                # prepositional phrase\n",
    "#     ADJ: {<JJ.*>}                 # adjective\n",
    "#     ADV: {<RB.*>}                 # adverb\n",
    "#     CONJ: {<CC>}                  # conjunction\n",
    "#     PUNC: {<\\.|,|:|''|\\(``|\\)|\\$>}  # punctuation\n",
    "#     # Define sentence patterns\n",
    "#     S: {<NP><VP>}                 # simple sentence\n",
    "    \n",
    "# \"\"\"\n",
    "\n",
    "grammar = r\"\"\"\n",
    "    NP: {<DT>?<JJ>*<NN|NNS|NNP|NNPS>+}       # noun phrase (requires at least one noun)\n",
    "    VP: {<VB|VBD|VBG|VBN|VBP|VBZ><NP>+}     # verb phrase (requires at least one noun phrase)\n",
    "    PP: {<IN><NN|NNS|NNP|NNPS>}             # prepositional phrase (requires a noun)\n",
    "    ADJ: {<JJ|JJR|JJS>}                     # adjective (matches only common adjective types)\n",
    "    ADV: {<RB|RBR|RBS>}                     # adverb (matches only common adverb types)\n",
    "    CONJ: {<CC>}                            # conjunction (matches only coordinating conjunctions)\n",
    "    PUNC: {<\\.|,|:|;|\\?|!>}                 # punctuation (matches common punctuation symbols)\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "# sentences = [\n",
    "#     \"Mary had a little lamb.\",\n",
    "#     \"John has a cute black pup.\",\n",
    "#     \"I ate five apples.\",\n",
    "#     \"I saw her duck\",\n",
    "# ]\n",
    "\n",
    "def has_noun_phrase(sentence):\n",
    "    parsed = chunkParser.parse(pos_tag(word_tokenize(sentence)))\n",
    "    for subtree in parsed:\n",
    "        if type(subtree) == nltk.Tree and subtree.label() == 'NP':\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "chunkParser = nltk.RegexpParser(grammar)\n",
    "for sentence in sentences:\n",
    "    if(has_noun_phrase(sentence)):\n",
    "        print(sentence)\n",
    "    # print(has_noun_phrase(sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The word 'tear' is ambiguous in the sentence 'She had a tear'.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'She had a Tear drop'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "def disambiguate(sentence):\n",
    "    # Tokenize the sentence into individual words\n",
    "    tokens = nltk.word_tokenize(sentence)\n",
    "    \n",
    "    # Part-of-speech tag each word\n",
    "    pos_tags = nltk.pos_tag(tokens)\n",
    "    \n",
    "    # Identify the possible meanings of the sentence based on the parts of speech\n",
    "    for i, (word, pos) in enumerate(pos_tags):\n",
    "        if pos == 'NN' or pos == 'VB':\n",
    "            # Check if the word is a homograph\n",
    "            if len(nltk.corpus.wordnet.synsets(word)) > 1:\n",
    "                # If the word is a homograph, prompt the user to clarify the intended meaning\n",
    "                print(\"The word '{}' is ambiguous in the sentence '{}'.\".format(word, sentence))\n",
    "                clarification = input(\"Please clarify the intended meaning: \")\n",
    "                \n",
    "                # Replace the ambiguous word with the user's clarification\n",
    "                tokens[i] = clarification\n",
    "    \n",
    "    # Reconstruct the sentence from the tokens\n",
    "    corrected_sentence = ' '.join(tokens)\n",
    "    \n",
    "    return corrected_sentence\n",
    "\n",
    "disambiguate(text)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
