{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter a sentence: I saw her duck\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.tree import ParentedTree\n",
    "from nltk import word_tokenize, pos_tag, RegexpParser\n",
    "from nltk.tree import Tree\n",
    "\n",
    "sentence = input(\"Enter a sentence: \")\n",
    "\n",
    "# Tokenize the sentence and extract the part of speech tags\n",
    "tokens = word_tokenize(sentence)\n",
    "pos_tags = pos_tag(tokens)\n",
    "\n",
    "words = nltk.word_tokenize(sentence)\n",
    "\n",
    "# Identify the part-of-speech tags for the words\n",
    "pos_tags = nltk.pos_tag(words)\n",
    "\n",
    "# Define the grammar for the parser\n",
    "grammar = r\"\"\"\n",
    "    NP: {<DT|PP\\$>?<JJ>*<NN>}   # chunk determiner/possessive, adjectives and noun\n",
    "        {<NNP>+}                # chunk sequences of proper nouns\n",
    "        {<PRP>}\n",
    "    VP: {<VB.*>+}               # chunk verbs and their modals\n",
    "        {<JJ>*<RB>?<VB.*>+}     # chunk adverbs modifying the verb\n",
    "    PP: {<IN><NP>}              # chunk prepositions followed by noun phrases\n",
    "\"\"\"\n",
    "\n",
    "# Create the parser and parse the sentence\n",
    "parser = RegexpParser(grammar)\n",
    "tree = parser.parse(pos_tags)\n",
    "\n",
    "# Visualize the parse tree\n",
    "tree.draw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('I', 'PRP')\n",
      "('saw', 'VBD')\n",
      "('her', 'PRP')\n",
      "('duck', 'NN')\n"
     ]
    }
   ],
   "source": [
    "for i in pos_tags:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I => nsubj\n",
      "saw => ROOT\n",
      "her => poss\n",
      "duck => dobj\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" xml:lang=\"en\" id=\"48c18712cb7d4b458bd221fa61e67ab5-0\" class=\"displacy\" width=\"750\" height=\"312.0\" direction=\"ltr\" style=\"max-width: none; height: 312.0px; color: #000000; background: #ffffff; font-family: Arial; direction: ltr\">\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"50\">I</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"50\">PRON</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"225\">saw</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"225\">VERB</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"400\">her</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"400\">PRON</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"575\">duck</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"575\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-48c18712cb7d4b458bd221fa61e67ab5-0-0\" stroke-width=\"2px\" d=\"M70,177.0 C70,89.5 220.0,89.5 220.0,177.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-48c18712cb7d4b458bd221fa61e67ab5-0-0\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">nsubj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M70,179.0 L62,167.0 78,167.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-48c18712cb7d4b458bd221fa61e67ab5-0-1\" stroke-width=\"2px\" d=\"M420,177.0 C420,89.5 570.0,89.5 570.0,177.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-48c18712cb7d4b458bd221fa61e67ab5-0-1\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">poss</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M420,179.0 L412,167.0 428,167.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-48c18712cb7d4b458bd221fa61e67ab5-0-2\" stroke-width=\"2px\" d=\"M245,177.0 C245,2.0 575.0,2.0 575.0,177.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-48c18712cb7d4b458bd221fa61e67ab5-0-2\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">dobj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M575.0,179.0 L583.0,167.0 567.0,167.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "</svg></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "text = sentence\n",
    "doc = nlp(text)\n",
    "\n",
    "# Getting dependency tags\n",
    "for token in doc:\n",
    "    print(token.text, '=>', token.dep_)\n",
    "\n",
    "# Importing visualizer\n",
    "from spacy import displacy\n",
    "\n",
    "# Visualizing dependency tree\n",
    "displacy.render(doc, jupyter=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The word 'duck' is ambiguous in the sentence 'I saw her duck'.\n",
      "Ambiguous word: duck\n",
      "  Meaning 1: dip\n",
      "  Meaning 2: fudge\n",
      "  Meaning 3: douse\n",
      "  Meaning 4: duck\n",
      "  Meaning 5: parry\n",
      "  Meaning 6: evade\n",
      "  Meaning 7: sidestep\n",
      "  Meaning 8: hedge\n",
      "  Meaning 9: circumvent\n",
      "  Meaning 10: skirt\n",
      "  Meaning 11: duck's_egg\n",
      "  Meaning 12: dodge\n",
      "  Meaning 13: put_off\n",
      "  Meaning 14: elude\n"
     ]
    }
   ],
   "source": [
    "aw = []\n",
    "ambiguous_words = []\n",
    "sentences = []\n",
    "\n",
    "for i, (word, pos) in enumerate(pos_tags):\n",
    "    w=\"\"\n",
    "    if pos == 'NN' or pos == 'VB':\n",
    "        # Check if the word is a homograph (similar words with a different meaning)\n",
    "        \n",
    "        if len(nltk.corpus.wordnet.synsets(word)) > 1:\n",
    "            print(\"The word '{}' is ambiguous in the sentence '{}'.\".format(\n",
    "                word, sentence))\n",
    "            aw.append(word)\n",
    "            w=word\n",
    "\n",
    "        # Identify the possible meanings for any verbs or nouns with multiple senses\n",
    "        meanings = set()\n",
    "        for synset in wordnet.synsets(word):\n",
    "            for lemma in synset.lemmas():\n",
    "                meanings.add(lemma.name())\n",
    "        if len(meanings) > 1:\n",
    "            print(f\"Ambiguous word: {word}\")\n",
    "            for j, meaning in enumerate(meanings):\n",
    "                ambiguous_words.append(meaning)\n",
    "                print(f\"  Meaning {j+1}: {meaning}\") \n",
    "    # Generate all possible sentences by replacing ambiguous words\n",
    "    for meanings in ambiguous_words:\n",
    "            new_sentence = sentence.replace(word, meanings)\n",
    "            sentences.append(new_sentence) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I saw her dip\n",
      "I saw her fudge\n",
      "I saw her douse\n",
      "I saw her duck\n",
      "I saw her parry\n",
      "I saw her evade\n",
      "I saw her sidestep\n",
      "I saw her hedge\n",
      "I saw her circumvent\n",
      "I saw her skirt\n",
      "I saw her duck's_egg\n",
      "I saw her dodge\n",
      "I saw her put_off\n",
      "I saw her elude\n"
     ]
    }
   ],
   "source": [
    "for i in sentences:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I saw her dip\n",
      "I saw her fudge\n",
      "I saw her douse\n",
      "I saw her duck\n",
      "I saw her parry\n",
      "I saw her evade\n",
      "I saw her sidestep\n",
      "I saw her hedge\n",
      "I saw her circumvent\n",
      "I saw her skirt\n",
      "I saw her duck's_egg\n",
      "I saw her dodge\n",
      "I saw her put_off\n",
      "I saw her elude\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import pos_tag\n",
    "\n",
    "# grammar = ('''\n",
    "#     NP: {<NNP><VBZ|VBD><DT><JJ>*<NN><.>}\n",
    "#     ''')\n",
    "\n",
    "# grammar = r\"\"\"\n",
    "#     NP: {<DT>?<JJ>*<NN.*>+}       # noun phrase\n",
    "#     VP: {<VB.*><NP|PP>*}          # verb phrase\n",
    "#     PP: {<IN><NP>}                # prepositional phrase\n",
    "#     ADJ: {<JJ.*>}                 # adjective\n",
    "#     ADV: {<RB.*>}                 # adverb\n",
    "#     CONJ: {<CC>}                  # conjunction\n",
    "#     PUNC: {<\\.|,|:|''|\\(``|\\)|\\$>}  # punctuation\n",
    "#     # Define sentence patterns\n",
    "#     S: {<NP><VP>}                 # simple sentence\n",
    "    \n",
    "# \"\"\"\n",
    "\n",
    "grammar = r\"\"\"\n",
    "    NP: {<DT>?<JJ>*<NN|NNS|NNP|NNPS>+}       # noun phrase (requires at least one noun)\n",
    "    VP: {<VB|VBD|VBG|VBN|VBP|VBZ><NP>+}     # verb phrase (requires at least one noun phrase)\n",
    "    PP: {<IN><NN|NNS|NNP|NNPS>}             # prepositional phrase (requires a noun)\n",
    "    ADJ: {<JJ|JJR|JJS>}                     # adjective (matches only common adjective types)\n",
    "    ADV: {<RB|RBR|RBS>}                     # adverb (matches only common adverb types)\n",
    "    CONJ: {<CC>}                            # conjunction (matches only coordinating conjunctions)\n",
    "    PUNC: {<\\.|,|:|;|\\?|!>}                 # punctuation (matches common punctuation symbols)\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "# sentences = [\n",
    "#     \"Mary had a little lamb.\",\n",
    "#     \"John has a cute black pup.\",\n",
    "#     \"I ate five apples.\",\n",
    "#     \"I saw her duck\",\n",
    "# ]\n",
    "\n",
    "def has_noun_phrase(sentence):\n",
    "    parsed = chunkParser.parse(pos_tag(word_tokenize(sentence)))\n",
    "    for subtree in parsed:\n",
    "        if type(subtree) == nltk.Tree and subtree.label() == 'NP':\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "chunkParser = nltk.RegexpParser(grammar)\n",
    "for sentence in sentences:\n",
    "    if(has_noun_phrase(sentence)):\n",
    "        print(sentence)\n",
    "    # print(has_noun_phrase(sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The word 'duck' is ambiguous in the sentence 'I saw her duck'.\n",
      "Please clarify the intended meaning: duck belonging to her\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'I saw her duck belonging to her'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "def disambiguate(sentence):\n",
    "    # Tokenize the sentence into individual words\n",
    "    tokens = nltk.word_tokenize(sentence)\n",
    "    \n",
    "    # Part-of-speech tag each word\n",
    "    pos_tags = nltk.pos_tag(tokens)\n",
    "    \n",
    "    # Identify the possible meanings of the sentence based on the parts of speech\n",
    "    for i, (word, pos) in enumerate(pos_tags):\n",
    "        if pos == 'NN' or pos == 'VB':\n",
    "            # Check if the word is a homograph\n",
    "            if len(nltk.corpus.wordnet.synsets(word)) > 1:\n",
    "                # If the word is a homograph, prompt the user to clarify the intended meaning\n",
    "                print(\"The word '{}' is ambiguous in the sentence '{}'.\".format(word, sentence))\n",
    "                clarification = input(\"Please clarify the intended meaning: \")\n",
    "                \n",
    "                # Replace the ambiguous word with the user's clarification\n",
    "                tokens[i] = clarification\n",
    "    \n",
    "    # Reconstruct the sentence from the tokens\n",
    "    corrected_sentence = ' '.join(tokens)\n",
    "    # Tokenize the sentence and extract the part of speech tags\n",
    "    tokens2 = word_tokenize(corrected_sentence)\n",
    "    pos_tags2 = pos_tag(tokens2)\n",
    "\n",
    "    words2 = nltk.word_tokenize(corrected_sentence)\n",
    "\n",
    "    # Identify the part-of-speech tags for the words\n",
    "    pos_tags2 = nltk.pos_tag(words2)\n",
    "\n",
    "    # Define the grammar for the parser\n",
    "    grammar2 = r\"\"\"\n",
    "        NP: {<DT|PP\\$>?<JJ>*<NN>}   # chunk determiner/possessive, adjectives and noun\n",
    "            {<NNP>+}                # chunk sequences of proper nouns\n",
    "            {<PRP>}\n",
    "        VP: {<VB.*>+}               # chunk verbs and their modals\n",
    "            {<JJ>*<RB>?<VB.*>+}     # chunk adverbs modifying the verb\n",
    "        PP: {<IN><NP>}              # chunk prepositions followed by noun phrases\n",
    "    \"\"\"\n",
    "\n",
    "    # Create the parser and parse the sentence\n",
    "    parser2 = RegexpParser(grammar2)\n",
    "    tree2 = parser2.parse(pos_tags2)\n",
    "\n",
    "    # Visualize the parse tree\n",
    "    tree2.draw()\n",
    "    return corrected_sentence\n",
    "\n",
    "disambiguate(text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
